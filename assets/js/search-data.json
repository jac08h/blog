{
  
    
        "post0": {
            "title": "Recognizing hand-written digits - SGD from scratch",
            "content": "Inspired by chapter 4 in The fastai book. Check it out for more detailed description of the steps. . The book discusses problem of classifying 3s and 7s. Here, I tackle the same classification, but with all ten digits. . Objective . Design a model that can classify hand-written digits from MNIST dataset. . Steps . Download, sort, show . Download the dataset. . path = untar_data(URLs.MNIST) . (path/&#39;training&#39;).ls() . (#10) [Path(&#39;training/0&#39;),Path(&#39;training/2&#39;),Path(&#39;training/9&#39;),Path(&#39;training/8&#39;),Path(&#39;training/7&#39;),Path(&#39;training/1&#39;),Path(&#39;training/5&#39;),Path(&#39;training/4&#39;),Path(&#39;training/6&#39;),Path(&#39;training/3&#39;)] . Store the image paths to a nested list. We can access the images for every digit by indexing the container. Since the python indexing starts from 0, the indexes conveniently follow the underlying digits. . digits = [] for i in range(10): current_digit_path = (path/&#39;training&#39;/str(i)) current_digit = current_digit_path.ls().sorted() digits.append(current_digit) . twos = digits[2] im2_path = twos[1] im2 = Image.open(im2_path) im2 . The images are represented by pixels, each of which holds the value from 0 (black) to 255 (white), with gradients of gray between. For example, 20 would be almost black. . We can see the underlying representation of images by converting them to NumPy array... . array(im2)[2:7,6:12] . array([[ 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 81, 131, 152], [ 0, 0, 14, 209, 253, 242], [ 0, 0, 25, 228, 95, 0], [ 0, 0, 11, 41, 0, 0]], dtype=uint8) . ...or by creating a Pandas dataframe from tensor, and then plotting the gray-scale using the pixel values. . im2_t = tensor(im2) df = pd.DataFrame(im2_t[2:15,6:20]) df.style.set_properties(**{&#39;font-size&#39;:&#39;6pt&#39;}).background_gradient(&#39;Greys&#39;) . 0 1 2 3 4 5 6 7 8 9 10 11 12 13 . 0 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 1 0 | 0 | 0 | 81 | 131 | 152 | 194 | 194 | 225 | 98 | 5 | 0 | 0 | 0 | . 2 0 | 0 | 14 | 209 | 253 | 242 | 242 | 242 | 251 | 254 | 183 | 9 | 0 | 0 | . 3 0 | 0 | 25 | 228 | 95 | 0 | 0 | 0 | 113 | 250 | 254 | 219 | 24 | 0 | . 4 0 | 0 | 11 | 41 | 0 | 0 | 0 | 0 | 0 | 80 | 210 | 254 | 167 | 22 | . 5 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 75 | 254 | 254 | 113 | . 6 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 28 | 223 | 254 | 161 | . 7 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 124 | 254 | 223 | . 8 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 82 | 254 | 254 | . 9 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 82 | 254 | 254 | . 10 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 82 | 254 | 254 | . 11 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 115 | 254 | 229 | . 12 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 23 | 220 | 254 | 161 | . Measuring the difference: Pixel similarity . One straight-forward way to categorize the images would be to measure pixel similarity. . That is, for every digit, we take all of the images from the training set, and for each pixel we calculate the average value. . To classify new number, we compare each pixel of this new number with the corresponding &quot;average&quot; pixel of a digit. We classify it as a digit where the difference was smallest. . Creating &quot;average&quot; digits . First, we convert the digit images to tensors. . digit_tensors = [] for d_images in digits: digit_tensors.append([tensor(Image.open(o)) for o in d_images]) . for d_tensors in digit_tensors: print(len(d_tensors), end=&quot;, &quot;) . 5923, 6742, 5958, 6131, 5842, 5421, 5918, 6265, 5851, 5949, . To calculate the average pixel values of a digit, we need to stack all of the corresponding digits to a single three-dimensional tensor - rank-3 tensor. . stacked_digit_tensors = [] for t in digit_tensors: stacked_current = torch.stack(t).float()/255 stacked_digit_tensors.append(stacked_current) . And calculate the mean. . digit_means = [] for s_d in stacked_digit_tensors: digit_means.append(s_d.mean(0)) . We can see how an &quot;ideal&quot; 2 looks like. . show_image(digit_means[2]) . &lt;AxesSubplot:&gt; . And the whole range. . show_images(digit_means) . To recap, this digits are just average values of every pixel for a given digit. . Measuring the similarity . There are two main ways to measure the similarity of the images: . Mean of the absolute value of differences - mean absolute difference - L1 norm | Mean of the square of differences - root mean squared error - L2 norm | The main difference is that the latter penalizes bigger differences more. . We&#39;re going to use both techniques. We start by picking a random two digit. Then, compare it to an ideal 2 and ideal 4. . a_2 = stacked_digit_tensors[2][0] mean_2 = digit_means[2] mean_4 = digit_means[4] show_images([a_2, mean_2, mean_4]) . L1-norm with our random 2: . dist_2_abs = (a_2 - mean_2).abs().mean() dist_2_abs . tensor(0.1350) . a_2 - mean_2 returns a tensor - differences for every pixel. Then, mean is calculated from absolute values of the differences. . show_image(a_2 - mean_2) . &lt;AxesSubplot:&gt; . L2-norm: . dist_2_sqr = ((a_2 - mean_2)**2).mean() dist_2_sqr . tensor(0.0547) . Now, compare the random 2 to ideal 4. . dist_4_abs = (a_2 - mean_4).abs().mean() dist_4_sqr = ((a_2 - mean_4)**2).mean() dist_4_abs, dist_4_sqr . (tensor(0.1750), tensor(0.1036)) . In both cases, the distance to the mean-4 is greater than the distance to the mean 2 - so the model would pick the right number. . PyTorch comes with both of these loss functions in torch.nn.functional as l1_loss and mse_loss. . F.l1_loss(a_2.float(),mean_2), F.mse_loss(a_2,mean_2).sqrt() . (tensor(0.1350), tensor(0.2338)) . Calculating accuracy . Now, we would like to see how good the predictions actually are. We can find out by using digits from validation set and comparing the predictions to actual values. . validation_digits = [] for i in range(10): valid_current = torch.stack([tensor(Image.open(o)) for o in (path/&#39;testing&#39;/str(i)).ls()]) valid_current = valid_current.float()/255 validation_digits.append(valid_current) . validation_digits[3].shape . torch.Size([1010, 28, 28]) . A function to calculate distance between two tensors. It utilizes broadcasting - automatically expanding smaller rank tensor if the ranks of tensor arguments don&#39;t match. . def mnist_distance(a,b): return (a-b).abs().mean((-1,-2)) . Same rank: . mnist_distance(a_2, mean_2) . tensor(0.1350) . Non-matching rank: . mnist_distance(validation_digits[2], mean_2) . tensor([0.1275, 0.1369, 0.1477, ..., 0.1503, 0.1582, 0.1609]) . Note that this doesn&#39;t actually create new copies of mean_2 - it just behaves like it did. . We&#39;ll use this mnist_distance to classify images. We guess the image to represent number where it had smallest distance from the mean image. . def is_target(tns, target): target_mean = digit_means[target] other_means = digit_means[:target] + digit_means[target+1:] intermed_tns = tensor([True]) for o_mean in other_means: target_dist = mnist_distance(tns, target_mean) o_dist = mnist_distance(tns, o_mean) x = target_dist &lt; o_dist intermed_tns = torch.logical_and(intermed_tns, x) return intermed_tns . acc_sum = 0 for i in range(10): accuracy = is_target(stacked_digit_tensors[i], i).float().mean() print(f&quot;{i}: {accuracy*100:.2f}%&quot;) acc_sum += float(accuracy) print(f&quot;Mean accuracy: {acc_sum/10*100:.2f}%&quot;) . 0: 81.41% 1: 99.81% 2: 43.29% 3: 58.36% 4: 66.48% 5: 30.01% 6: 74.54% 7: 77.78% 8: 39.58% 9: 71.63% Mean accuracy: 64.29% . As you can see, the predictions are all over the place, but the result is not bad - more than 6 times better than random guesses. Note: Extremely high accuracy with predicting 1s is a little unsettling and looks like a bug - if it is, I wasn&#39;t able to locate it. . Stochastic Gradient Descent . SGD: Find local minima of a loss function by changing the parameters. Adjust the rate at which each parameter changes by calculating gradients (steepness of the slope). . The stochastic me . Steps . Initialize weights randomly | Calculate the predictions | Calculate the loss | Calculate the gradients | Step the weights | Repeat | End | Before we begin, we create a dataset that contains digit tensor, label pairs. . train_x = torch.cat(stacked_digit_tensors).view(-1, 28*28) labels = [] for i in range(10): labels += [i] * len(digits[i]) train_y = tensor(labels).unsqueeze(-1) train_x.shape, train_y.shape . (torch.Size([60000, 784]), torch.Size([60000, 1])) . dset = list(zip(train_x,train_y)) x,y = dset[0] x.shape, y.shape . (torch.Size([784]), torch.Size([1])) . Same for validation set: . valid_x = torch.cat([t for t in validation_digits]).view(-1, 28*28) labels = [] for i in range(10): labels += [i] * len(validation_digits[i]) valid_y = tensor(labels).unsqueeze(1) valid_dset = list(zip(valid_x, valid_y)) . Step 1: Initialize the parameters randomly . def init_params(size, std=1.0): return (torch.randn(size)*std).requires_grad_() . We want the weights to output in 10 dimensions - 1 for each digit. . weights = init_params((28*28, 10)) . If all the pixels would be 0, no matter the weights, the function would return zero. To avoid this, we include bias - that is the b in w*x+b. . bias = init_params(10) . Step 2: Calculate the predictions . We can calculate the prediction now: . train_x[0]*weights.T + bias . tensor([[1.3673, 1.3673, 1.3673, ..., 1.3673, 1.3673, 1.3673], [1.3673, 1.3673, 1.3673, ..., 1.3673, 1.3673, 1.3673], [1.3673, 1.3673, 1.3673, ..., 1.3673, 1.3673, 1.3673], ..., [1.3673, 1.3673, 1.3673, ..., 1.3673, 1.3673, 1.3673], [1.3673, 1.3673, 1.3673, ..., 1.3673, 1.3673, 1.3673], [1.3673, 1.3673, 1.3673, ..., 1.3673, 1.3673, 1.3673]], grad_fn=&lt;AddBackward0&gt;) . However, this would be extremely slow. We&#39;ll use matrix multiplication to calculate w*x for every row in a matrix. . def linear1(xb): return (xb@weights + bias) preds = linear1(train_x) preds[0], preds.shape . (tensor([-11.9867, -2.5228, 5.5697, 8.3973, 3.4768, 8.1628, -5.4304, -0.2353, 0.3871, 14.0976], grad_fn=&lt;SelectBackward&gt;), torch.Size([60000, 10])) . Step 3: Calculate the loss . We can calculate the accuracy of our model now. . def p_to_digit(x): return x.argmax(-1) def accuracy(xb, yb): return (p_to_digit(xb) == yb).float().mean() . To do that, we need to convert the prediction tensor to a digit by picking up a digit with highest confidence. . Then we just compare if the predicted digit is equal to the label. . To test this, pick two numbers with labels and calculate their predictions. . two_numbers = tensor(train_x[10000:10002]) two_labels = tensor(train_y[10000:10002]) two_preds = linear1(two_numbers) . two_preds[0] . tensor([ 1.0390, 5.1063, -3.8192, -6.3767, -7.0070, -1.9540, 5.3849, -3.0133, 16.6652, 13.6806], grad_fn=&lt;SelectBackward&gt;) . p_to_digit(two_preds), two_labels . (tensor([8, 8]), tensor([[1], [1]])) . accuracy(two_preds, two_labels) . tensor(0.) . We get around ~10% accuracy, as would expect. We want to use gradients to improve our model using SGD. We could use accuracy as our loss function. But there&#39;s a problem. Gradients are calculated over the slope of the function (rise over run). Now, some changes to the parameters might not be enough to change the prediction to different number. . We need a loss function that changes every time we tweak the parameters. . We can do this by predicting not only the digit, but also the confidence of the prediction. We use softmax to keep the predictions in [0,1] interval proportional to the original values. . Before writing the loss function, we need to come up with a way to convert labels to tensor expressing confidences for every digit. That is, it should have a value 1 for the target digit, and 0 for every other digit. The loss will use this tensor to calculate the distance from the desired answer. . def label_to_tensor(l): l = l.type(torch.int64) return (torch.zeros(l.shape[0], 10)).scatter_(1, l, 1) . def mnist_loss(predictions, targets): predictions = predictions.softmax(-1) targets = label_to_tensor(targets) return (predictions - targets).abs().mean() . Step 4: Calculate the gradients . Calculating the loss one item at a time would be too slow, but doing it all at once would take a long time. We settle for a compromise: mini-batches - calculation are taken on a batch of items at a time. This also utilizes GPUs capabilities to perform tasks in parallel. . dl = DataLoader(dset, batch_size=256) valid_dl = DataLoader(valid_dset, batch_size=256) weights = init_params((28*28,10)) bias = init_params(10) . def calc_grad(xb, yb, model): preds = model(xb) loss = mnist_loss(preds, yb) loss.backward() . Training . When training, we step in the opposite direction of the gradient to move to the function minima. | Don&#39;t forget to zero out the gradients to avoid them being added together after multiple runs. | Learning rate is used to adjust the size of the steps - to augment the gradients impact. | . | . def train_epoch(model, lr, params): for xb,yb in dl: calc_grad(xb, yb, model) for p in params: p.data -= p.grad*lr p.grad.zero_() . def validate_epoch(model): accs = [accuracy(model(xb), yb) for xb,yb in valid_dl] return round(torch.stack(accs).mean().item(), 4) . validate_epoch(linear1) . 0.5307 . With random weights, the ~10% shouldn&#39;t surprise us. But does it actually get better if we train the model? . lr = 1. params = weights,bias train_epoch(linear1, lr, params) validate_epoch(linear1) . 0.5482 . It does! . We can train some more. . lr = 1. params = weights,bias for i in range(20): train_epoch(linear1, lr, params) print(validate_epoch(linear1), end=&#39; &#39;) . 0.1203 0.159 0.1953 0.2227 0.2399 0.2582 0.2864 0.3095 0.3335 0.3662 0.3946 0.4182 0.4366 0.4558 0.4678 0.4833 0.4968 0.5108 0.5203 0.5307 . The accuracy after a few training cycles reaches ~50%, which is quite good, for such a simple model! . Although the pixel similarity model worked better, this is a basic block which we can build on. And it&#39;s actually learning. . Creating an Optimizer . Unsurprisingly, there&#39;s bunch of PyTorch classes that make this easier to implement. nn.Linear is a combination of our init_params and linear1. . linear_model = nn.Linear(28*28,10) linear_model . Linear(in_features=784, out_features=10, bias=True) . A purpose of the optimizer is to minimize the loss function by stepping the weights, as we&#39;ve done in calc_grad and train_epoch. . fastai provides SGD, which does essentially the same as we did. . We can use Learner to put it all together. . dls = DataLoaders(dl, valid_dl) . learn = Learner(dls, nn.Linear(28*28,10), opt_func=SGD, loss_func=mnist_loss, metrics=accuracy) . learn.fit(20, lr=1.) . epoch train_loss valid_loss accuracy time . 0 | 0.173718 | 0.161770 | 0.189957 | 00:01 | . 1 | 0.162735 | 0.132883 | 0.351541 | 00:01 | . 2 | 0.124754 | 0.138107 | 0.308111 | 00:01 | . 3 | 0.093411 | 0.133728 | 0.330376 | 00:01 | . 4 | 0.064736 | 0.121844 | 0.392798 | 00:01 | . 5 | 0.056113 | 0.113167 | 0.437181 | 00:01 | . 6 | 0.052612 | 0.106790 | 0.461695 | 00:01 | . 7 | 0.050625 | 0.101881 | 0.483196 | 00:01 | . 8 | 0.049248 | 0.097861 | 0.500118 | 00:01 | . 9 | 0.047335 | 0.094604 | 0.515318 | 00:01 | . 10 | 0.044942 | 0.092674 | 0.524771 | 00:01 | . 11 | 0.043550 | 0.088731 | 0.541892 | 00:01 | . 12 | 0.042572 | 0.085538 | 0.556468 | 00:01 | . 13 | 0.041895 | 0.082740 | 0.568823 | 00:01 | . 14 | 0.041335 | 0.080248 | 0.581545 | 00:01 | . 15 | 0.040875 | 0.077980 | 0.592765 | 00:01 | . 16 | 0.040487 | 0.075885 | 0.603075 | 00:01 | . 17 | 0.040153 | 0.073942 | 0.611133 | 00:01 | . 18 | 0.039864 | 0.072142 | 0.619703 | 00:01 | . 19 | 0.039610 | 0.070477 | 0.627218 | 00:01 | . This performs slightly better than our model. My guess would be that SGD optimizer is slightly more advanced than ours. I&#39;ll update this when I find out. . We can advance to crucial step - adding more layers. . More layers and nonlinearity . If we added only another (linear) layer, those two layers could be modified (by using calculus) to be represented as a single linear function. The solution is to add a nonlinear function between the two. . Now, it&#39;s not possible to simplify it to single linear function. . simple_net = nn.Sequential( nn.Linear(28*28,30), # linear layer nn.ReLU(), # nonlinearity nn.Linear(30,10) # linear layer ) . nn.ReLU is essentialy max(0, x). It turns any negative numbers to zero, leaving positive numbers untouched. . plot_function(F.relu) . learn = Learner(dls, simple_net, opt_func=SGD, loss_func=mnist_loss, metrics=accuracy) . learn.fit(20,0.1) . epoch train_loss valid_loss accuracy time . 0 | 0.179586 | 0.177974 | 0.248417 | 00:01 | . 1 | 0.178430 | 0.173071 | 0.288831 | 00:01 | . 2 | 0.175726 | 0.163790 | 0.289710 | 00:01 | . 3 | 0.166170 | 0.154028 | 0.283608 | 00:01 | . 4 | 0.145490 | 0.150887 | 0.263941 | 00:01 | . 5 | 0.129021 | 0.141877 | 0.308289 | 00:01 | . 6 | 0.121218 | 0.129039 | 0.389680 | 00:01 | . 7 | 0.114690 | 0.119177 | 0.421859 | 00:01 | . 8 | 0.104323 | 0.112369 | 0.435302 | 00:01 | . 9 | 0.096131 | 0.106515 | 0.456612 | 00:01 | . 10 | 0.091037 | 0.101304 | 0.482437 | 00:01 | . 11 | 0.087791 | 0.096685 | 0.502814 | 00:01 | . 12 | 0.085579 | 0.092664 | 0.524015 | 00:01 | . 13 | 0.083983 | 0.089234 | 0.541441 | 00:01 | . 14 | 0.082785 | 0.086336 | 0.555548 | 00:01 | . 15 | 0.081860 | 0.083887 | 0.566695 | 00:01 | . 16 | 0.081115 | 0.081837 | 0.575562 | 00:01 | . 17 | 0.080491 | 0.080116 | 0.582145 | 00:01 | . 18 | 0.079965 | 0.078663 | 0.586118 | 00:01 | . 19 | 0.079511 | 0.077425 | 0.590228 | 00:01 | . The results are slightly disappointing - the accuracy after 20 cycles is similar to our single-layer model. What happens if we add another layer? . bigger_net = nn.Sequential( nn.Linear(28*28,30), nn.ReLU(), nn.Linear(30, 30), nn.ReLU(), nn.Linear(30, 10), ) . b_learn = Learner(dls, bigger_net, opt_func=SGD, loss_func=mnist_loss, metrics=accuracy) . b_learn.fit(20, 0.1) . epoch train_loss valid_loss accuracy time . 0 | 0.181227 | 0.179791 | 0.109768 | 00:01 | . 1 | 0.180957 | 0.179483 | 0.130466 | 00:01 | . 2 | 0.180724 | 0.179031 | 0.140161 | 00:01 | . 3 | 0.180577 | 0.178252 | 0.141316 | 00:01 | . 4 | 0.180701 | 0.176359 | 0.134754 | 00:01 | . 5 | 0.180448 | 0.172006 | 0.167983 | 00:01 | . 6 | 0.178882 | 0.168110 | 0.183042 | 00:01 | . 7 | 0.176265 | 0.164861 | 0.186179 | 00:01 | . 8 | 0.169956 | 0.162534 | 0.176538 | 00:01 | . 9 | 0.160770 | 0.163873 | 0.166075 | 00:01 | . 10 | 0.154921 | 0.163730 | 0.164884 | 00:01 | . 11 | 0.150058 | 0.159350 | 0.183652 | 00:01 | . 12 | 0.143657 | 0.154351 | 0.205950 | 00:01 | . 13 | 0.138707 | 0.149301 | 0.219642 | 00:01 | . 14 | 0.144044 | 0.130155 | 0.361669 | 00:01 | . 15 | 0.135130 | 0.125586 | 0.381818 | 00:01 | . 16 | 0.135763 | 0.115760 | 0.419380 | 00:01 | . 17 | 0.127948 | 0.113278 | 0.421662 | 00:01 | . 18 | 0.118951 | 0.112473 | 0.419497 | 00:01 | . 19 | 0.113066 | 0.110320 | 0.426027 | 00:01 | . The model got even worse! . What is going on here? Frankly, I don&#39;t know. Perhaps I should pick other learning rate? Or my loss function is off. Maybe I&#39;m missing something wholly different. . But quick search reveals there might be other problems: . overfitting caused by more layers | problem with initializing the weights | problem with gradients (e.g. vanishing gradient) | . Since I&#39;m still only at the beginning of my ML journey, I can&#39;t easily solve the problem. I hope to update this blog when I&#39;ll find out what&#39;s the deal here. . Conclusion . In this blog, we&#39;ve gone through comparing images by pixel similarity to working neural network learning through SGD. Even if the results were a bit disappointing, it was clear that the model learned and gradually improved the performance - even with starting out with completely random values! . I was surprised by the simplicity of the SGD. You pick a random weights and the nudge them into the right direction to minimize the loss function, and you can adjust the &quot;nudge&quot; by how steep the change is. So cool! .",
            "url": "https://jac08h.github.io/blog/programming/fastai/2020/09/24/digits-classifier.html",
            "relUrl": "/programming/fastai/2020/09/24/digits-classifier.html",
            "date": " • Sep 24, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Snake Classifier",
            "content": "Objective . Create a neural network model that&#39;s capable of deciding between two snake categories. . Using chapter 2 from The fastai book as a guide. Follow the chapter for more detailed explanations. . from fastbook import * from fastai.vision.widgets import * . key = &#39;XXX&#39; # bing image search key snake_types = &#39;kingsnake&#39;, &#39;coral snake&#39; path = Path(&#39;snakes&#39;) . Download 150 images of each snake category to corresponding folders. . if not path.exists(): path.mkdir() for snake_type in snake_types: dest = (path/snake_type) dest.mkdir(exist_ok=True) results = search_images_bing(key, f&#39;{snake_type}&#39;) download_images(dest, urls=results.attrgot(&#39;content_url&#39;)) . Download of http://thenaturaltraveller.files.wordpress.com/2014/01/img_9112.jpg has failed after 5 retries Fix the download manually: $ mkdir -p snakes/coral snake $ cd snakes/coral snake $ wget -c http://thenaturaltraveller.files.wordpress.com/2014/01/img_9112.jpg $ tar xf img_9112.jpg And re-run your code once the download is successful . fns = get_image_files(path) . Remove images that can&#39;t be opened. . failed = verify_images(fns) failed.map(Path.unlink); . Create a DataBlock. Split the images to train (80%) and validation set (20%). Create synthetic images by augmenting the original data using aug_transforms. . snakes = DataBlock( blocks = (ImageBlock, CategoryBlock), get_items=get_image_files, splitter = RandomSplitter(valid_pct=0.2, seed=42), get_y=parent_label, item_tfms=RandomResizedCrop(224, min_scale=0.5), batch_tfms=aug_transforms(mult=2) ) . dls = snakes.dataloaders(path) . dls.train.show_batch(max_n=8, nrows=2, unique=False) . Train the model. . Using pre-trained model resnet18 as a starting point, then fine-tuning it to the snakes problem. . learn = cnn_learner(dls, resnet18, metrics=error_rate) learn.fine_tune(4) . epoch train_loss valid_loss error_rate time . 0 | 1.324692 | 0.415803 | 0.172414 | 00:05 | . epoch train_loss valid_loss error_rate time . 0 | 0.603712 | 0.305789 | 0.068965 | 00:05 | . 1 | 0.562530 | 0.206492 | 0.103448 | 00:05 | . 2 | 0.509871 | 0.188923 | 0.051724 | 00:06 | . 3 | 0.483981 | 0.185373 | 0.068965 | 00:05 | . The error rate stabilizes at around 6%. . interp = ClassificationInterpretation.from_learner(learn) interp.plot_confusion_matrix() . Export the model. . learn.export() . path = Path() path.ls(file_exts=&#39;.pkl&#39;) . (#1) [Path(&#39;export.pkl&#39;)] . You can find out how the model performes on your images on this. The code to setup the web interface is hosted here. . Conclusion . With no more than 150 pictures of a snake of one kind, using pre-trained model with data augmentation of the original pictures, and not doing any tweaking of the parameters, the model achieves accuracy higher than 90%. . I&#39;d like to emphasize the fact that the way we managed this is by showing the model bunch of labeled images. That&#39;s it. We didn&#39;t code any significant features of the snake kinds, like colors or size of the eyes. Fascinating, isn&#39;t it? .",
            "url": "https://jac08h.github.io/blog/programming/fastai/2020/09/15/snake-classifier.html",
            "relUrl": "/programming/fastai/2020/09/15/snake-classifier.html",
            "date": " • Sep 15, 2020"
        }
        
    
  
    
  

  
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page9": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://jac08h.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}